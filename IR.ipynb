{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Who did Barack Obama campaign against in 2008\"\n",
      "('the question type is:', array([u'HUM'], dtype=object))\n",
      "Only low confidence answer found\n",
      "('Ans text: ', ' In 2004 , Barack Obama received national attention during Barack campaign to represent Illinois in the United States Senate with Barack victory in the March Democratic Party primary . Barack began Barack presidential campaign in 2007 and , after a close primary campaign against Hillary Rodham Clinton in 2008 , Barack won sufficient delegates in the Democratic Party primaries to receive the presidential nomination .')\n"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "import spacy \n",
    "import scipy \n",
    "import sklearn \n",
    "import nltk \n",
    "import pickle\n",
    "import time\n",
    "from sklearn import svm \n",
    "from sklearn.svm import LinearSVC \n",
    "from scipy import sparse \n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk import corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#start2 = time.clock()\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def pre_process(dta): \n",
    "    return pandas.get_dummies(dta) \n",
    "\n",
    " \n",
    "def transform_data_matrix(df_question_train, df_question_predict): \n",
    "\n",
    " \n",
    "    df_question_train_columns = list(df_question_train.columns) \n",
    "    df_question_predict_columns = list(df_question_predict.columns) \n",
    "\n",
    " \n",
    "    df_question_trans_columns = list(set(df_question_train_columns + df_question_predict_columns)) \n",
    "\n",
    " \n",
    "    trans_data_train = {} \n",
    "\n",
    " \n",
    "    for feature in df_question_trans_columns: \n",
    "        if feature not in df_question_train: \n",
    "            trans_data_train[feature] = [0 for i in range(len(df_question_train.index))] \n",
    "        else: \n",
    "            trans_data_train[feature] = list(df_question_train[feature]) \n",
    "\n",
    " \n",
    "    df_question_train = pandas.DataFrame(trans_data_train) \n",
    "    df_question_train = csr_matrix(df_question_train) \n",
    "\n",
    " \n",
    "    trans_data_predict = {} \n",
    "\n",
    " \n",
    "    for feature in trans_data_train: \n",
    "        if feature not in df_question_predict: \n",
    "            trans_data_predict[feature] = 0 \n",
    "        else: \n",
    "            trans_data_predict[feature] = list(df_question_predict[feature])   \n",
    "\n",
    " \n",
    "    df_question_predict = pandas.DataFrame(trans_data_predict) \n",
    "    df_question_predict = csr_matrix(df_question_predict) \n",
    "\n",
    " \n",
    "    return df_question_train, df_question_predict \n",
    "\n",
    " \n",
    "def get_question_predict_data(en_doc): \n",
    "    sentence_list = list(en_doc.sents)[0:1] \n",
    "    en_nlp = spacy.load(\"en_core_web_md\") \n",
    "\n",
    " \n",
    "    question_data_frame = [] \n",
    "\n",
    " \n",
    "    for sentence in sentence_list: \n",
    "\n",
    " \n",
    "        wh_bi_gram = [] \n",
    "        root_token, wh_pos, wh_nbor_pos, wh_word = [\"\"] * 4 \n",
    "        for token in sentence: \n",
    "\n",
    " \n",
    "        # if token is of WH question type \n",
    "            if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\": \n",
    "                wh_pos = token.tag_ \n",
    "                wh_word = token.text \n",
    "                wh_bi_gram.append(token.text) \n",
    "                wh_bi_gram.append(str(en_doc[token.i + 1])) \n",
    "                wh_nbor_pos = en_doc[token.i + 1].tag_ \n",
    "     \n",
    "            # if token is the root of sentence \n",
    "            if token.dep_ == \"ROOT\": \n",
    "                root_token = token.tag_ \n",
    "     \n",
    "        question_data_frame_obj = {'WH': wh_word, 'WH-POS': wh_pos, 'WH-NBOR-POS': wh_nbor_pos, 'Root-POS': root_token} \n",
    "        question_data_frame.append(question_data_frame_obj) \n",
    "         \n",
    "        df_question = pandas.DataFrame(question_data_frame) \n",
    "\n",
    " \n",
    "    return df_question \n",
    "\n",
    " \n",
    "def support_vector_machine(df_question_train, df_question_class, df_question_predict): \n",
    "    lin_clf = LinearSVC() \n",
    "    lin_clf.fit(df_question_train, df_question_class) \n",
    "    prediction = lin_clf.predict(df_question_predict) \n",
    "    return prediction, lin_clf \n",
    "\n",
    " \n",
    "def classify_question(en_doc): \n",
    "    \n",
    "    f = open('training_data_set.pkl', 'rb')\n",
    "    df_question_train = pickle.load(f)    \n",
    "    f.close()\n",
    "    \n",
    "    f1 = open('training_data_classes.pkl', 'rb')\n",
    "    df_question_class = pickle.load(f1)    \n",
    "    f1.close() \n",
    "     \n",
    "    df_question_predict = get_question_predict_data(en_doc) \n",
    "     \n",
    "    df_question_predict = pre_process(df_question_predict) \n",
    "     \n",
    "    df_question_train, df_question_predict = transform_data_matrix(df_question_train, df_question_predict) \n",
    "    predicted_class, svc_clf = support_vector_machine(df_question_train, df_question_class, df_question_predict) \n",
    "    return predicted_class \n",
    "\n",
    "     \n",
    "def make_min(sentence,Qtype): \n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in (stopwords.words('english') or \"?\")])\n",
    "    #start_=process(sentence)\n",
    "    \n",
    "    sentenceParts = [] \n",
    "     \n",
    "    if Qtype == \"q\": \n",
    "        for i in range(1,len(sentence.strip(\"?\").split())): \n",
    "            sentenceParts.append(sentence.strip(\"?\").split()[i])\n",
    "        sentence1 = ' '.join([word for word in sentenceParts])\n",
    "        sentence1 = en_nlp(u'' + sentence1)\n",
    "        sentenceParts = []\n",
    "        for token in sentence1:\n",
    "            sentenceParts.append(token.lemma_)\n",
    "        \n",
    "    else: \n",
    "        for each in sentence.strip(\"?\").split():\n",
    "            sentenceParts.append(each)\n",
    "        sentence1 = ' '.join([word for word in sentenceParts])\n",
    "        sentence1 = en_nlp(u'' + sentence1)\n",
    "        sentenceParts = []\n",
    "        for token in sentence1:\n",
    "            sentenceParts.append(token.lemma_)\n",
    "    \n",
    "    return sentenceParts\n",
    "  \n",
    "def findAnAns(question): \n",
    "    corpusFile = open(\"resolved.txt\",\"r\") \n",
    "    data = corpusFile.read() \n",
    "    corpusFile.close() \n",
    " \n",
    "    data = data.split(\".\" or \"!\" or \"?\") \n",
    " \n",
    "    linedata = [] \n",
    "    questionParts = make_min(question,\"q\") \n",
    "\n",
    "    confidenceLevels = [] \n",
    "    for sent in data: \n",
    "        confidenceLevel = [] \n",
    "        matched = 0 \n",
    "        answerParts = make_min(sent, \"a\") \n",
    "        for each in questionParts: \n",
    "            if each in answerParts: \n",
    "                matched += 1 \n",
    "        confidenceLevel.append(sent) \n",
    "        confidenceLevel.append(matched*100/len(questionParts)) \n",
    "        confidenceLevels.append(confidenceLevel)\n",
    "    \n",
    "    v = sorted(confidenceLevels, key=lambda x: x[1], reverse=True)\n",
    "    #print(v)\n",
    "    return v\n",
    "  \n",
    "def getasentence(sortedconfidencelevels):\n",
    "    newlist1 = []\n",
    "    combinedsentence = ''\n",
    "    newlist2 = []\n",
    "    sum_ = 0\n",
    "    for ans in sortedconfidencelevels:\n",
    "        if ans[1]>60:\n",
    "            newlist1.append(ans)\n",
    "    #print(newlist1)\n",
    "    if len(newlist1)==0:\n",
    "        return(sortedconfidencelevels[0])\n",
    "    else:\n",
    "        for i in newlist1:\n",
    "            combinedsentence += str(i[0])\n",
    "            combinedsentence +=\".\"\n",
    "            sum_ += i[1]\n",
    "        newlist2 = [combinedsentence, (sum_)/len(newlist1)]\n",
    "        return newlist2, newlist1\n",
    "      \n",
    "def checkifCorrectAnswer(question, answer):\n",
    "    en_doc_1 = en_nlp(u'' + question)\n",
    "    en_doc_2 = en_nlp(u'' + answer)\n",
    "    c = 0\n",
    "    for token in en_doc_1:\n",
    "      if token.dep_ == \"ROOT\":\n",
    "          target_root = token.text\n",
    "          target_root = token.lemma_\n",
    "          #print (target_root)\n",
    "          syns = wordnet.synsets(target_root)\n",
    "          synonyms = []\n",
    "          for i in range(0,len(syns)):\n",
    "              synonyms.append(syns[i].lemmas()[0].name())\n",
    "          #print(\"synonyms\",synonyms)\n",
    "          \n",
    "    for token in en_doc_2:\n",
    "        token = token.lemma_\n",
    "        #print (token)\n",
    "        if token in synonyms:\n",
    "            c = 1\n",
    "           \n",
    "        \n",
    "    if c == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def get_exact_answer(question, answersent, possiblereplies):\n",
    "    person_list = []\n",
    "    reqd_list = []\n",
    "    entity_list = []\n",
    "    location_list = []\n",
    "    number_list =[]\n",
    "    list_ = []\n",
    "    en_doc_3 = en_nlp(u'' + answersent)\n",
    "    en_doc_4 = en_nlp(u''+question)\n",
    "    if question_class == [u'HUM']:\n",
    "        for ent in en_doc_3.ents:\n",
    "            #print(ent.text, ent.label_)\n",
    "            if ent.label_ == \"PERSON\" or ent.label_ == \"NORP\" or ent.label_ ==\"ORG\":\n",
    "                person_list.append(ent.text)\n",
    "        reqd_list = person_list\n",
    "    \n",
    "    #print(person_list)\n",
    "    if question_class == [u'NUM']:\n",
    "        for ent in en_doc_3.ents:\n",
    "            #print(ent.text, ent.label_)\n",
    "            if ent.label_ == \"DATE\" or ent.label_==\"PERCENT\" or ent.label_==\"TIME\" or ent.label_==\"QUANTITY\" or ent.label_==\"ORDINAL\" or ent.label_==\"CARDINAL\":\n",
    "                number_list.append(ent.text)\n",
    "        reqd_list = number_list\n",
    "    if question_class == [u\"LOC\"]:\n",
    "        for ent in en_doc_3.ents:\n",
    "            if ent.label_ == \"LOC\" or ent.label_==\"GPE\" or ent.label_==\"FAC\" or ent.label_ == \"ORG\":\n",
    "                location_list.append(ent.text)\n",
    "        reqd_list = location_list\n",
    "    if question_class == [u'ENTY']:\n",
    "        with open('picklepath.pkl','rb') as f:\n",
    "            total_entity_list = pickle.load(f)\n",
    "            #print (total_entity_list)\n",
    "        for token in en_doc_3:\n",
    "            if token.text in total_entity_list:\n",
    "                entity_list.append(token.text)\n",
    "        for ent in en_doc_3.ents:\n",
    "            if ent.text in total_entity_list:\n",
    "                entity_list.append(ent.text)\n",
    "        #print(entity_list)\n",
    "        reqd_list = entity_list\n",
    "    \n",
    "    for i in reqd_list:\n",
    "        for token in en_doc_4:\n",
    "            if str(i) == token.text:\n",
    "                reqd_list.remove(i)\n",
    "                #break\n",
    "        for ent in en_doc_4.ents:\n",
    "            #print(ent.text)\n",
    "            if str(i) == ent.text:\n",
    "                if i in reqd_list:\n",
    "                    reqd_list.remove(i)\n",
    "    print(\"possible answers:\", reqd_list)\n",
    "    \n",
    "            \n",
    "    if len(reqd_list) == 1:\n",
    "        return reqd_list\n",
    "    elif len(reqd_list)!=1 and len(reqd_list)!=0:\n",
    "        for token in en_doc_4:\n",
    "            if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\":\n",
    "                target_dep = token.dep_\n",
    "                target_head = token.head.text\n",
    "        \n",
    "        for token in en_doc_3:#and token.head.text== target_head:\n",
    "            #print (\"Hey\", token.text, token.dep_, token.head.text)\n",
    "            if token.text in reqd_list:\n",
    "                #print(token.text)\n",
    "                if token.dep_ == target_dep:\n",
    "                    #print(token.text, token.head.text)\n",
    "                    syns = wordnet.synsets(target_head)\n",
    "                    synonyms1 = []\n",
    "                    for i in range(0,len(syns)):\n",
    "                        synonyms1.append(syns[i].lemmas()[0].name())\n",
    "                    #print(synonyms1)                \n",
    "                    if token.head.lemma_ in synonyms1:\n",
    "                        return token.text       \n",
    "                    else:\n",
    "                        return None\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "#start1 = time.clock()-start2    \n",
    "if __name__ == \"__main__\":\n",
    "    question = input()\n",
    "    #start = time.clock()\n",
    "    question_class = classify_question(en_nlp(u'' + question))\n",
    "    #print(question_class)\n",
    "    print(\"the question type is:\", question_class)\n",
    "    listofsentences = findAnAns(question)\n",
    "    reply, possiblereplies= getasentence(listofsentences)\n",
    "    #reply = findAnAns(question)\n",
    "    #print(reply)\n",
    "    if reply[1] > 0: \n",
    "        checkifCorrectAnswer(question, reply[0])\n",
    "        if checkifCorrectAnswer(question, reply[0]) == True:\n",
    "            answer = get_exact_answer(question, reply[0], possiblereplies)\n",
    "            if answer!=None:\n",
    "                print(\"Ans: \", answer)\n",
    "                print (str(reply[1]) +\"% \"+ \"match\")\n",
    "            else:\n",
    "                print(\"Ans text: \", reply[0])\n",
    "                print (str(reply[1]) +\"% \"+ \"match\")\n",
    "        else:\n",
    "            print(\"Only low confidence answer found\")\n",
    "            print(\"Ans text: \", reply[0])\n",
    "    else:\n",
    "        print(\"invalid question\")\n",
    "    #print(\"Time taken\", time.clock()-start+start1)\n",
    "                                                                                                                                                                                                                                                                              \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
