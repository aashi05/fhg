{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What did Obama execute\"\n",
      "invalid question\n"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "import spacy \n",
    "import scipy \n",
    "import sklearn \n",
    "import nltk \n",
    " \n",
    "from sklearn import svm \n",
    "from sklearn.svm import LinearSVC \n",
    "from scipy import sparse \n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk import corpus, stem\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def remove_irrelevant_features(df_question): \n",
    "    df_question_class = df_question.pop('Class') \n",
    "\n",
    " \n",
    "    df_question.pop('Question') \n",
    "    #df_question.pop('WH-Bigram') \n",
    "\n",
    " \n",
    "    return df_question_class \n",
    "\n",
    " \n",
    "def pre_process(dta): \n",
    "    return pandas.get_dummies(dta) \n",
    "\n",
    " \n",
    "def transform_data_matrix(df_question_train, df_question_predict): \n",
    "\n",
    " \n",
    "    df_question_train_columns = list(df_question_train.columns) \n",
    "    df_question_predict_columns = list(df_question_predict.columns) \n",
    "\n",
    " \n",
    "    df_question_trans_columns = list(set(df_question_train_columns + df_question_predict_columns)) \n",
    "\n",
    " \n",
    "    trans_data_train = {} \n",
    "\n",
    " \n",
    "    for feature in df_question_trans_columns: \n",
    "        if feature not in df_question_train: \n",
    "            trans_data_train[feature] = [0 for i in range(len(df_question_train.index))] \n",
    "        else: \n",
    "            trans_data_train[feature] = list(df_question_train[feature]) \n",
    "\n",
    " \n",
    "    df_question_train = pandas.DataFrame(trans_data_train) \n",
    "    df_question_train = csr_matrix(df_question_train) \n",
    "\n",
    " \n",
    "    trans_data_predict = {} \n",
    "\n",
    " \n",
    "    for feature in trans_data_train: \n",
    "        if feature not in df_question_predict: \n",
    "            trans_data_predict[feature] = 0 \n",
    "        else: \n",
    "            trans_data_predict[feature] = list(df_question_predict[feature])   \n",
    "\n",
    " \n",
    "    df_question_predict = pandas.DataFrame(trans_data_predict) \n",
    "    df_question_predict = csr_matrix(df_question_predict) \n",
    "\n",
    " \n",
    "    return df_question_train, df_question_predict \n",
    "\n",
    " \n",
    "def get_question_predict_data(en_doc): \n",
    "    sentence_list = list(en_doc.sents)[0:1] \n",
    "    en_nlp = spacy.load(\"en_core_web_md\") \n",
    "\n",
    " \n",
    "    question_data_frame = [] \n",
    "\n",
    " \n",
    "    for sentence in sentence_list: \n",
    "\n",
    " \n",
    "        wh_bi_gram = [] \n",
    "        root_token, wh_pos, wh_nbor_pos, wh_word = [\"\"] * 4 \n",
    "        for token in sentence: \n",
    "\n",
    " \n",
    "        # if token is of WH question type \n",
    "            if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\": \n",
    "                wh_pos = token.tag_ \n",
    "                wh_word = token.text \n",
    "                wh_bi_gram.append(token.text) \n",
    "                wh_bi_gram.append(str(en_doc[token.i + 1])) \n",
    "                wh_nbor_pos = en_doc[token.i + 1].tag_ \n",
    "     \n",
    "            # if token is the root of sentence \n",
    "            if token.dep_ == \"ROOT\": \n",
    "                root_token = token.tag_ \n",
    "     \n",
    "        question_data_frame_obj = {'WH': wh_word, 'WH-POS': wh_pos, 'WH-NBOR-POS': wh_nbor_pos, 'Root-POS': root_token} \n",
    "        question_data_frame.append(question_data_frame_obj) \n",
    "         \n",
    "        df_question = pandas.DataFrame(question_data_frame) \n",
    "\n",
    " \n",
    "    return df_question \n",
    "\n",
    " \n",
    "def support_vector_machine(df_question_train, df_question_class, df_question_predict): \n",
    "    lin_clf = LinearSVC() \n",
    "    lin_clf.fit(df_question_train, df_question_class) \n",
    "    prediction = lin_clf.predict(df_question_predict) \n",
    "    return prediction, lin_clf \n",
    "\n",
    " \n",
    "def classify_question(en_doc): \n",
    "    training_data_path = \"qclassify.csv\" \n",
    "    df_question_train = pandas.read_csv(training_data_path, sep='|', header=0, encoding='cp1252') \n",
    "     \n",
    "    df_question_class = remove_irrelevant_features(df_question_train) \n",
    "    df_question_predict = get_question_predict_data(en_doc) \n",
    "    df_question_train = pre_process(df_question_train) \n",
    "    df_question_predict = pre_process(df_question_predict) \n",
    "     \n",
    "    df_question_train, df_question_predict = transform_data_matrix(df_question_train, df_question_predict) \n",
    "    predicted_class, svc_clf = support_vector_machine(df_question_train, df_question_class, df_question_predict) \n",
    "    return predicted_class \n",
    "\n",
    "def process(question): \n",
    "    if question.lower().startswith(\"who \" or \"when \" or \"where \" or \"what \" or \"why \" or \"is \" or \"was \" or \"will \" or \"are \" or \"were \" or \"do \" or \"does \" or \"did \" or \"have \" or \"has \" or \"can \"): \n",
    "        return 1 \n",
    "    elif question.lower().startswith(\"how \"): \n",
    "        return 2 \n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "      \n",
    "def make_min(sentence,Qtype): \n",
    "    sentence = sentence.upper() \n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in (stopwords.words('english'))]) \n",
    "    sentenceParts = [] \n",
    "    ps = PorterStemmer() \n",
    "    if Qtype == \"q\": \n",
    "        for i in range(process(sentence),len(sentence.strip(\"?\").split())): \n",
    "            sentenceParts.append(ps.stem(sentence.strip(\"?\").split()[i])) \n",
    "    else: \n",
    "        for each in sentence.strip(\"?\").split(): \n",
    "            sentenceParts.append(ps.stem(each)) \n",
    "    return sentenceParts\n",
    "  \n",
    "def findAnAns(question): \n",
    "    corpusFile = open(\"corpus.txt\",\"r\") \n",
    "    data = corpusFile.read() \n",
    "    corpusFile.close() \n",
    " \n",
    "    data = data.split(\".\" or \"!\" or \"?\") \n",
    " \n",
    "    linedata = [] \n",
    "    questionParts = make_min(question,\"q\") \n",
    "\n",
    "    confidenceLevels = [] \n",
    "    for sent in data: \n",
    "        confidenceLevel = [] \n",
    "        matched = 0 \n",
    "        answerParts = make_min(sent, \"a\") \n",
    "        for each in questionParts: \n",
    "            if each in answerParts: \n",
    "                matched += 1 \n",
    "        confidenceLevel.append(sent) \n",
    "        confidenceLevel.append(matched*100/len(questionParts)) \n",
    "        confidenceLevels.append(confidenceLevel)\n",
    "    \n",
    "    for ans in sorted(confidenceLevels, key=lambda x: x[1], reverse=True): \n",
    "        if question_class == [u'HUM']:\n",
    "            return ans \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    question = input()\n",
    "    question_class = classify_question(en_nlp(u'' + question))\n",
    "    if question_class == [u'HUM']:\n",
    "        print (question)\n",
    "        reply = findAnAns(question) \n",
    "        if reply[1] > 0: \n",
    "            print (reply[0]) \n",
    "            print (str(reply[1]) + \"%\" + \" confidence\") \n",
    "        else: \n",
    "            print (\"Only low confidence answer found\") \n",
    "            print (\"Ans\",answer) \n",
    "            print (str(matched*100/len(questionParts)) +\"% \"+ \"confidence\")\n",
    "    else:\n",
    "        print(\"invalid question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
