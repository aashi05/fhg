{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3b18556696ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m      \u001b[0mread_input_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainingdata.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qclassifynew.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3b18556696ed>\u001b[0m in \u001b[0;36mread_input_file\u001b[0;34m(raw_data_file, training_data_path, en_nlp)\u001b[0m\n\u001b[1;32m     15\u001b[0m              \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m              \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m              \u001b[0mquestion_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_class_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m              \u001b[0mprocess_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_nlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_fp_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import csv\n",
    "en_nlp = spacy.load(\"en_core_web_md\") \n",
    "\n",
    "def read_input_file(raw_data_file, training_data_path, en_nlp): \n",
    " \n",
    " \n",
    "     with open(training_data_path, 'a') as csv_fp: \n",
    "         csv_fp_writer = csv.writer(csv_fp, delimiter='|') \n",
    "         f = open(raw_data_file,'r') \n",
    "         f1=f.readlines() \n",
    "         for row in f1: \n",
    "             list_row = row.split(\" \") \n",
    "             question_class_list = list_row[0].split(\":\") \n",
    "             question = \" \".join(list_row[1:len(list_row)]) \n",
    "             question = question.strip(\"\\n\") \n",
    "             question_class = question_class_list[0] \n",
    "             \n",
    "             process_question(question, question_class, en_nlp, training_data_path, csv_fp_writer) \n",
    " \n",
    " \n",
    "         csv_fp.close() \n",
    "\n",
    "def process_question(question, question_class, en_nlp, training_data_path, csv_fp_writer): \n",
    "     en_doc = en_nlp(u''+ question) \n",
    "     sentence_list = list(en_doc.sents) \n",
    " \n",
    " \n",
    "     # Currently question classifier classifies only the 1st sentence of the question \n",
    "     sentence = sentence_list[0] \n",
    " \n",
    " \n",
    "     wh_bi_gram = [] \n",
    "     root_token, wh_pos, wh_nbor_pos, wh_word = [\"\"] * 4 \n",
    " \n",
    " \n",
    "     for token in sentence: \n",
    " \n",
    " \n",
    "         # if token is of WH question type \n",
    "         if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\": \n",
    "             wh_pos = token.tag_ \n",
    "             wh_word = token.text \n",
    "             wh_bi_gram.append(token.text) \n",
    "             wh_bi_gram.append(str(en_doc[token.i + 1])) \n",
    "             wh_nbor_pos = en_doc[token.i + 1].tag_ \n",
    " \n",
    " \n",
    "         # if token is the root of sentence \n",
    "         if token.dep_ == \"ROOT\": \n",
    "             root_token = token.tag_ \n",
    " \n",
    " \n",
    "     if wh_word != \"\" and \" \".join(wh_bi_gram) != \"\" and wh_pos != \"\" and wh_nbor_pos != \"\": \n",
    "         csv_fp_writer.writerow([question, wh_word, \" \".join(wh_bi_gram), wh_pos, wh_nbor_pos, root_token, question_class]) \n",
    "           \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     read_input_file('trainingdata.txt', 'qclassify.csv', en_nlp)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
